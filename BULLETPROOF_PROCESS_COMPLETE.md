# MAIA Bulletproof Process - Complete System Management

**Date:** December 17, 2025
**Status:** âœ… PRODUCTION READY
**Sovereignty:** 100% Voice Pipeline

---

## Quick Start (For Daily Use)

### Start Everything
```bash
cd /Users/soullab/MAIA-SOVEREIGN
./scripts/start-maia-full.sh
```

This single command starts:
- âœ… Whisper server (local STT)
- âœ… Ollama service (local intelligence)
- âœ… MAIA dev server
- âœ… Health checks for all services

### Check System Health
```bash
./scripts/check-whisper.sh
```

### Stop Whisper Server
```bash
./scripts/stop-whisper.sh
```

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MAIA Bulletproof Stack                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Layer 1: Voice Input (100% Sovereign)
  â”œâ”€ Browser MediaRecorder API
  â”œâ”€ WhisperContinuousConversation.tsx
  â””â”€ Whisper-cpp Server (Port 8080)
      â”œâ”€ Model: ggml-base.en.bin (147 MB)
      â”œâ”€ GPU: Apple M4 Max (Metal)
      â””â”€ Management: scripts/start-whisper.sh

Layer 2: Intelligence (100% Sovereign)
  â”œâ”€ Ollama Service (Port 11434)
  â”œâ”€ Model: DeepSeek-R1
  â””â”€ Consciousness Systems (local)

Layer 3: Voice Output (Approved External)
  â””â”€ OpenAI TTS API (user-approved for "speaking")

Layer 4: Application
  â”œâ”€ Next.js Dev Server (Port 3003)
  â””â”€ MAIA Interface: http://localhost:3003/maia
```

---

## Management Scripts

### 1. start-maia-full.sh (Master Startup)

**Purpose:** Start all MAIA services in correct order with health checks

**Features:**
- âœ… Checks for existing services before starting
- âœ… Waits for each service to be healthy
- âœ… Auto-recovers from stale processes
- âœ… Verifies GPU acceleration
- âœ… Checks required models
- âœ… Comprehensive status output

**Usage:**
```bash
./scripts/start-maia-full.sh
```

**Output Example:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                           â•‘
â•‘           MAIA SOVEREIGN - FULL SYSTEM STARTUP            â•‘
â•‘                                                           â•‘
â•‘             100% Sovereign Voice Conversation             â•‘
â•‘                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[SECTION] Step 1: Starting Whisper Server
[INFO] Starting Whisper server...
[INFO] âœ… Whisper server is fully operational!

[SECTION] Step 2: Checking Ollama Service
[INFO] âœ… Ollama service is running
[INFO] âœ… DeepSeek-R1 model found

[SECTION] Step 3: Checking Database Services
[INFO] âœ… PostgreSQL running on port 5432

[SECTION] Step 4: Starting MAIA Dev Server
[INFO] âœ… Dev server is ready!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  âœ… ALL SYSTEMS OPERATIONAL                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŒŸ MAIA Voice System Status:

  ğŸ¤ Speech-to-Text:   http://127.0.0.1:8080 (whisper-cpp, 100% sovereign)
  ğŸ§  Intelligence:      http://localhost:11434 (Ollama DeepSeek-R1, 100% sovereign)
  ğŸ¯ MAIA Interface:    http://localhost:3003/maia
  ğŸ”Š Text-to-Speech:    OpenAI TTS (approved for voice output)
```

---

### 2. start-whisper.sh (Whisper Server Management)

**Purpose:** Start Whisper server with comprehensive error handling

**Features:**
- âœ… Pre-flight checks (installation, model, FFmpeg)
- âœ… Kills existing conflicting processes
- âœ… Starts server in background with logging
- âœ… Performs HTTP health check
- âœ… Retries on failure (max 3 attempts)
- âœ… Verifies GPU acceleration

**Usage:**
```bash
./scripts/start-whisper.sh
```

**What It Checks:**
1. whisper-server installation (Homebrew)
2. Model file exists (~whisper-models/ggml-base.en.bin)
3. FFmpeg available (for audio conversion)
4. Port 8080 availability
5. Server HTTP responsiveness
6. GPU Metal support

**Auto-Recovery:**
- Kills conflicting processes on port 8080
- Retries startup up to 3 times
- 5-second delay between retries

---

### 3. stop-whisper.sh (Graceful Shutdown)

**Purpose:** Stop Whisper server gracefully

**Features:**
- âœ… Sends SIGTERM first (graceful)
- âœ… Waits up to 10 seconds
- âœ… Force kills if necessary (SIGKILL)
- âœ… Verifies port is free

**Usage:**
```bash
./scripts/stop-whisper.sh
```

**Output Example:**
```
==========================================
  Whisper Server Shutdown
==========================================

[INFO] Found process(es): 76662
[INFO] Sending SIGTERM to process 76662...
..........
[INFO] Process 76662 stopped successfully

==========================================
  âœ… Whisper server stopped successfully
==========================================
```

---

### 4. check-whisper.sh (Health Monitoring)

**Purpose:** Comprehensive health check and status report

**Features:**
- âœ… Checks port occupation
- âœ… Shows process details (CPU, memory, uptime)
- âœ… HTTP health check with response time
- âœ… Verifies model file
- âœ… Checks GPU support
- âœ… Shows recent log entries

**Usage:**
```bash
./scripts/check-whisper.sh
```

**Output Example:**
```
==========================================
  Whisper Server Health Check
==========================================

[STATUS] Checking port 8080...
[INFO] âœ… Process(es) found: 76662

[STATUS] Process Details (PID: 76662):
  PID:     76662
  User:    soullab
  CPU:     0.0%
  Memory:  0.7%
  Uptime:  13:16
  Command: whisper-server -m /Users/soullab/whisper-models/ggml-base.en.bin --host 127.0.0.1 --port 8080 --convert

[STATUS] Performing HTTP health check...
[INFO] âœ… Server is responding to HTTP requests
[INFO]    Response time: 0.000390s

[STATUS] Checking model file...
[INFO] âœ… Model found: /Users/soullab/whisper-models/ggml-base.en.bin (144M)

[STATUS] Checking GPU support...
[INFO] âœ… Metal GPU support available
[INFO]    GPU: Apple M4 Max

==========================================
[INFO] âœ… Overall Status: HEALTHY
==========================================
```

---

## Startup Sequence (What Happens)

### Phase 1: Whisper Server (Local STT)
1. Kill any conflicting processes on port 8080
2. Verify whisper-cpp installation
3. Verify model file exists (ggml-base.en.bin)
4. Check FFmpeg availability
5. Start server: `whisper-server -m ~/whisper-models/ggml-base.en.bin --host 127.0.0.1 --port 8080 --convert`
6. Wait for HTTP endpoint to respond
7. Log to: `whisper-server.log`

**Startup Time:** ~5-8 seconds (model loading + GPU initialization)

### Phase 2: Ollama Service (Local Intelligence)
1. Check if Ollama installed
2. Check if service responding (port 11434)
3. Start Ollama.app if needed (macOS)
4. Verify DeepSeek-R1 model available
5. Wait for API to respond

**Startup Time:** ~3-5 seconds (if already running)

### Phase 3: MAIA Dev Server (Application)
1. Remove stale lock files
2. Start Next.js: `PORT=3003 npm run dev`
3. Wait for HTTP endpoint (localhost:3003)
4. Verify MAIA interface loads
5. Log to: `dev-server.log`

**Startup Time:** ~10-15 seconds (Turbopack compilation)

---

## Error Recovery

### Scenario 1: Whisper Server Won't Start

**Symptoms:**
- Port 8080 shows as occupied but server not responding
- "Local Whisper transcription failed" in browser

**Fix:**
```bash
# Force kill everything on port 8080
kill -9 $(lsof -ti:8080)

# Restart cleanly
./scripts/start-whisper.sh
```

### Scenario 2: Model Not Found

**Symptoms:**
- Startup fails with "loading model from '...' failed"

**Fix:**
```bash
mkdir -p ~/whisper-models
cd ~/whisper-models
curl -L -O https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin
```

### Scenario 3: Dev Server Lock File

**Symptoms:**
- "Unable to acquire lock at .next/dev/lock"

**Fix:**
```bash
rm -f /Users/soullab/MAIA-SOVEREIGN/.next/dev/lock
PORT=3003 npm run dev
```

### Scenario 4: Ollama Not Responding

**Symptoms:**
- "Failed to connect to Ollama"

**Fix (macOS):**
```bash
# Restart Ollama app
killall Ollama
open -a Ollama

# Wait 5 seconds
sleep 5

# Verify
curl http://localhost:11434/api/tags
```

---

## Production Deployment (Auto-Start on Boot)

### Option 1: macOS LaunchAgent (Whisper Server)

Create `~/Library/LaunchAgents/com.soullab.whisper.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.soullab.whisper</string>

    <key>ProgramArguments</key>
    <array>
        <string>/opt/homebrew/bin/whisper-server</string>
        <string>-m</string>
        <string>/Users/soullab/whisper-models/ggml-base.en.bin</string>
        <string>--host</string>
        <string>127.0.0.1</string>
        <string>--port</string>
        <string>8080</string>
        <string>--convert</string>
    </array>

    <key>RunAtLoad</key>
    <true/>

    <key>KeepAlive</key>
    <true/>

    <key>StandardOutPath</key>
    <string>/Users/soullab/whisper-server.log</string>

    <key>StandardErrorPath</key>
    <string>/Users/soullab/whisper-server-error.log</string>
</dict>
</plist>
```

**Enable:**
```bash
launchctl load ~/Library/LaunchAgents/com.soullab.whisper.plist
launchctl start com.soullab.whisper
```

**Disable:**
```bash
launchctl stop com.soullab.whisper
launchctl unload ~/Library/LaunchAgents/com.soullab.whisper.plist
```

---

## Monitoring and Maintenance

### Check All Services
```bash
# Quick status
lsof -ti:8080 && echo "âœ… Whisper running" || echo "âŒ Whisper down"
lsof -ti:11434 && echo "âœ… Ollama running" || echo "âŒ Ollama down"
lsof -ti:3003 && echo "âœ… Dev server running" || echo "âŒ Dev server down"
```

### View Logs in Real-Time
```bash
# Whisper server logs
tail -f whisper-server.log

# Dev server logs
tail -f dev-server.log

# Combined
tail -f whisper-server.log -f dev-server.log
```

### Resource Usage
```bash
# Check Whisper memory usage
ps aux | grep whisper-server | grep -v grep

# Check all MAIA processes
ps aux | grep -E "whisper|ollama|next-server" | grep -v grep
```

---

## Daily Workflow

### Morning Startup
```bash
cd /Users/soullab/MAIA-SOVEREIGN
./scripts/start-maia-full.sh
```

**Expected Time:** ~20 seconds total

### During Development
```bash
# Check health periodically
./scripts/check-whisper.sh

# View logs if issues arise
tail -f whisper-server.log
```

### End of Day Shutdown
```bash
# Stop Whisper (optional, can leave running)
./scripts/stop-whisper.sh

# Stop dev server
killall next-server

# Stop Ollama (optional)
killall Ollama
```

---

## Troubleshooting Decision Tree

```
Is voice input working?
â”œâ”€ No â†’ Check Whisper server
â”‚   â”œâ”€ Run: ./scripts/check-whisper.sh
â”‚   â”œâ”€ Is server running?
â”‚   â”‚   â”œâ”€ No â†’ Run: ./scripts/start-whisper.sh
â”‚   â”‚   â””â”€ Yes â†’ Is server responding?
â”‚   â”‚       â”œâ”€ No â†’ Restart: ./scripts/stop-whisper.sh && ./scripts/start-whisper.sh
â”‚   â”‚       â””â”€ Yes â†’ Check browser console for errors
â”‚   â””â”€ Check endpoint: curl http://localhost:3003/api/voice/transcribe-simple
â””â”€ Yes â†’ Is MAIA responding?
    â”œâ”€ No â†’ Check Ollama
    â”‚   â”œâ”€ Run: curl http://localhost:11434/api/tags
    â”‚   â””â”€ If fails â†’ Start Ollama app
    â””â”€ Yes â†’ System fully operational âœ…
```

---

## Security Notes

### Whisper Server Security
- **Bind Address:** 127.0.0.1 (localhost only, NOT 0.0.0.0)
- **No Authentication:** Not exposed to network, safe for local use
- **Audio Privacy:** Audio never leaves local machine

### Production Considerations
If deploying to production server:
1. Add authentication to Whisper endpoint
2. Use HTTPS/TLS for all connections
3. Implement rate limiting
4. Add request logging and monitoring

---

## Performance Optimization

### Whisper Server Performance
- **Model Choice:** Base model (fastest, good accuracy)
- **GPU:** Metal acceleration (Apple Silicon)
- **Typical Latency:** 1-2 seconds for 5-second audio
- **Memory:** ~250 MB resident

### Upgrade to Large Model (Higher Accuracy)
```bash
# Download large-v3 model (3GB)
cd ~/whisper-models
curl -L -O https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v3.bin

# Update start script to use large model
# Edit: scripts/start-whisper.sh
# Change: WHISPER_MODEL="$HOME/whisper-models/ggml-large-v3.bin"
```

**Trade-offs:**
- âœ… Higher accuracy
- âŒ 3x slower (3-5 seconds per utterance)
- âŒ 3x more memory (~500 MB)

---

## Summary

You now have a **bulletproof** MAIA voice system with:

âœ… **One-command startup:** `./scripts/start-maia-full.sh`
âœ… **Health monitoring:** `./scripts/check-whisper.sh`
âœ… **Graceful shutdown:** `./scripts/stop-whisper.sh`
âœ… **Auto-recovery:** Handles conflicting processes, retries failures
âœ… **Comprehensive logging:** whisper-server.log, dev-server.log
âœ… **Production ready:** LaunchAgent support for auto-start
âœ… **100% Sovereign:** Local STT + local intelligence

**Zero external dependencies for transcription** (OpenAI only for TTS)

---

**ğŸŒŸ Status: BULLETPROOF - Ready for Daily Use âœ¨**
